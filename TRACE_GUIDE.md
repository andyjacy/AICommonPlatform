# 调用链追踪系统 - 使用指南

## 功能概述

调用链追踪系统允许用户在提出问题时，看到 AI 平台内部的完整处理流程，包括每一步的目的、调用的服务、数据流转等信息。这对于**学习和理解 AI 架构**非常有帮助。

---

## 核心特性

### 1. 智能追踪开关
- 在问答页面中，有一个"📊 显示调用链"复选框
- 勾选此框后，提问会返回完整的执行追踪信息
- 不勾选时，行为与普通问答相同

### 2. 8阶段处理流程

系统遵循行业标准的 AI 处理流程：

```
问题输入 → 意图识别 → 知识检索 → 上下文增强 → Prompt编排 → LLM推理 → 结果处理 → 结果返回
```

#### 详细说明：

| 阶段 | 服务 | 目的 | 关键技术 |
|------|------|------|---------|
| **1. 输入处理** | QA Entry | 接收和清洗用户输入 | NLP预处理、文本清洗 |
| **2. 意图识别** | QA Entry | 理解用户真实意图 | 文本分类、实体识别、关键词提取 |
| **3. 知识检索** | RAG Service | 从知识库检索相关信息 | 向量搜索(Milvus)、全文搜索(Elasticsearch) |
| **4. 上下文增强** | Integration Service | 获取实时企业数据 | ERP、CRM、HRM系统集成 |
| **5. Prompt编排** | Prompt Service | 设计高效的提示词 | 角色系统、Few-shot学习、Chain-of-Thought |
| **6. LLM推理** | LLM Service | 生成回答内容 | 多模型支持(GPT-4、通义千问等) |
| **7. 结果处理** | LLM Service | 格式化和优化结果 | 置信度评分、参考文献整理 |
| **8. 结果返回** | Web UI | 返回最终答案给用户 | 响应流式化、元数据记录 |

### 3. 架构学习模式

点击侧边栏的"🏗️ 查看 AI 架构"按钮，可以查看：
- 平台的微服务架构
- 8个核心处理阶段的详细说明
- 每个阶段使用的关键技术
- 支持的LLM模型列表
- 核心技术栈说明

---

## 使用示例

### 步骤 1: 启用追踪

1. 打开 Web UI: http://localhost:3000
2. 进入"问答"模块（默认打开）
3. 在输入框下方，勾选"📊 显示调用链"复选框

### 步骤 2: 提出问题

在问题输入框中输入问题，例如：
- "什么是检索增强生成(RAG)?"
- "我们公司最新的销售数据是什么?"
- "如何使用 Agent 工具链进行任务自动化?"

### 步骤 3: 查看追踪数据

提问后，系统会自动切换到"调用链追踪"标签页，显示：

```
【调用链信息】
追踪 ID: 472df420
总步骤: 12
总耗时: 0.123s
问题: 什么是检索增强生成(RAG)?

【处理流程详解】
Step 1 ✅ 输入处理 | QA Entry Service
  用途: 接收用户问题，进行文本预处理和清洗
  时间: 11:23:35
  数据: {"raw_question": "什么是检索增强生成(RAG)?"}

Step 2 ✅ 意图识别 | QA Entry Service
  用途: 进行问题分类（数据查询、操作执行等）和关键词提取
  时间: 11:23:35
  数据: {"intent": "数据查询", "keywords": ["销售", "数据"]}

... (更多步骤)

【架构说明】
【AI Common Platform 标准处理流程】

1. 输入处理阶段 (Input Processing)
   - 接收用户自然语言问题
   - 进行文本预处理和清洗
   ...
```

### 步骤 4: 查看架构信息

点击"🏗️ 查看 AI 架构"按钮，查看：
- 8个核心处理阶段
- 每个阶段支持的数据源
- 使用的关键技术和LLM模型

---

## 调用链追踪数据结构

### API 端点

#### 1. 带追踪的问答端点
```
POST /api/trace/qa/ask
Content-Type: application/json

{
  "question": "什么是 RAG?",
  "user_id": "user123"
}
```

**响应示例：**
```json
{
  "question": "什么是 RAG?",
  "answer": "...",
  "confidence": 0.94,
  "execution_time": 0.45,
  "sources": ["知识库", "企业数据库"],
  "trace": {
    "trace_id": "472df420",
    "question": "什么是 RAG?",
    "total_steps": 12,
    "total_time": "0.123s",
    "steps": [
      {
        "seq": 1,
        "timestamp": "2026-01-26T11:23:35.061059",
        "stage": "输入处理",
        "service": "QA Entry Service",
        "purpose": "接收用户问题，进行文本预处理和清洗",
        "status": "success",
        "data": {
          "raw_question": "什么是 RAG?"
        }
      },
      ... (更多步骤)
    ],
    "architecture_description": "【AI Common Platform 标准处理流程】\n\n1. 输入处理阶段..."
  }
}
```

#### 2. 架构信息端点
```
GET /api/trace/architecture
```

**响应包含：**
- `platform_name`: 平台名称
- `version`: 版本号
- `architecture.type`: 架构类型（微服务架构）
- `architecture.pattern`: 支持的模式（RAG、Agent、LLM）
- `architecture.core_stages`: 8个核心阶段的详细说明
- `key_technologies`: 关键技术栈

---

## 架构设计哲学

### 为什么需要调用链追踪？

1. **学习价值** 📚
   - 直观理解 AI 系统如何处理问题
   - 看到每一步的具体作用和数据变化
   - 学习行业标准的 AI 架构模式

2. **调试价值** 🔧
   - 快速定位问题发生在哪个阶段
   - 查看每个服务的输入输出数据
   - 理解各个服务之间的协作方式

3. **性能优化** ⚡
   - 识别耗时较长的处理阶段
   - 优化关键路径
   - 进行缓存和并行处理

4. **审计追溯** 📋
   - 记录完整的决策过程
   - 便于合规性检查
   - 追踪数据流和访问权限

---

## 技术栈说明

### 核心技术

| 组件 | 技术 | 说明 |
|------|------|------|
| **NLP预处理** | NLTK/jieba | 分词、词性标注、停用词处理 |
| **向量化** | Embeddings | Word2Vec/BERT/文本向量表示 |
| **向量数据库** | Milvus | 存储和检索高维向量，支持百万级数据 |
| **全文搜索** | Elasticsearch | 支持分词、模糊匹配、复杂查询 |
| **结构化数据** | PostgreSQL | 企业数据存储和复杂查询 |
| **缓存层** | Redis | 提高响应速度，支持会话管理 |
| **LLM模型** | 多源支持 | OpenAI/通义千问/文心一言/讯飞等 |
| **Web框架** | FastAPI | 高性能异步 API 框架 |

### 支持的LLM模型

- **OpenAI**: GPT-4, GPT-3.5-turbo
- **阿里**: 通义千问(Qwen)
- **百度**: 文心一言(Ernie)
- **科大讯飞**: 讯飞星火(SparkDesk)
- **智谱**: GLM-4
- **Meta**: Llama 2

---

## 常见问题

### Q1: 如何理解"上下文增强"阶段？
**A:** 这一步联系企业的 ERP、CRM、HRM 等系统，获取实时数据。例如，当用户问"今年销售怎样？"时，系统会从 ERP 中查询实时的销售数据，而不仅仅依赖知识库中的历史信息。

### Q2: RAG 和 Agent 有什么区别？
**A:** 
- **RAG（检索增强生成）**: 检索知识库，然后生成回答。适合需要知识库支持的场景。
- **Agent（代理）**: 可以自主调用多个工具和服务完成任务。适合复杂的多步骤任务。

### Q3: 为什么有些问题追踪步骤多，有些少？
**A:** 根据问题的复杂度，系统会采取不同的处理路径。简单问题可能跳过某些步骤（如需要查询复杂的企业数据），而复杂问题需要更多步骤。

### Q4: 我可以使用哪个LLM模型？
**A:** 通过 LLM Service 的配置，可以选择任何支持的模型。在 Prompt 编排阶段，会根据问题复杂度自动选择最合适的模型。

### Q5: 追踪数据会被保存吗？
**A:** 当前版本的追踪数据仅在内存中，会话结束后消失。如需持久化，可配置 Redis 或数据库存储。

---

## 学习路径

### 初级（理解基础概念）
1. ✅ 查看"AI 架构"了解整体结构
2. ✅ 提出简单问题，查看追踪流程
3. ✅ 理解 8 个阶段的作用

### 中级（深入理解核心技术）
1. ✅ 学习 RAG 的向量搜索原理
2. ✅ 理解 Prompt 工程的重要性
3. ✅ 探索不同 LLM 模型的差异

### 高级（优化和定制）
1. ✅ 修改 Prompt 模板，观察效果变化
2. ✅ 调整 RAG 的检索参数（top_k、相似度阈值）
3. ✅ 实现自定义的 Agent 工具

---

## 下一步

1. **启动系统**: `docker-compose -f docker-compose.lite.yml up -d`
2. **打开Web UI**: http://localhost:3000
3. **启用追踪**: 勾选"📊 显示调用链"
4. **开始探索**: 提出问题，观看完整处理流程！

---

**记住**: 每一条追踪链都是一次 AI 系统决策过程的完整记录。通过观察这些链，你可以深入理解现代 AI 应用的架构和原理！🚀

