import logging
import json
from typing import Optional, Dict, Any, List
from datetime import datetime
import re
import aiohttp
import asyncio

from config import Settings
from models import ClassificationResult, ContextData

logger = logging.getLogger(__name__)

class QuestionClassifier:
    """é—®é¢˜åˆ†ç±»å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.keywords_map = {
            "sales_inquiry": ["é”€å”®éƒ¨", "é”€å”®æ•°æ®", "é”€å”®é¢", "é”€å”®ç›®æ ‡", "ä¸šç»©", "æ”¶å…¥", "è¥æ”¶", "é”€å”®é‡"],
            "hr_inquiry": ["å‘˜å·¥", "äººåŠ›èµ„æº", "è–ªèµ„", "ç¦åˆ©", "è€ƒå‹¤", "æ‹›è˜", "HR", "äººäº‹"],
            "technical_inquiry": ["ç³»ç»Ÿ", "æ¶æ„", "æŠ€æœ¯", "ä»£ç ", "å¼€å‘", "ç¼–ç¨‹", "API", "æ¥å£"],
            "financial_inquiry": ["è´¢åŠ¡", "é¢„ç®—", "æˆæœ¬", "åˆ©æ¶¦", "è´¦æˆ·", "è´¢åŠ¡æŠ¥è¡¨", "æ”¶æ”¯"],
            "customer_inquiry": ["å®¢æˆ·", "å®¢æœ", "è®¢å•", "æŠ•è¯‰", "åé¦ˆ", "å’¨è¯¢"],
        }
    
    def classify(self, question: str) -> str:
        """
        å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»
        
        å‚æ•°ï¼š
        - question: é—®é¢˜æ–‡æœ¬
        
        è¿”å›ï¼š
        - question_type: é—®é¢˜ç±»å‹
        """
        question_lower = question.lower()
        
        # æŒ‰å…³é”®è¯é•¿åº¦æ’åºï¼ˆä¼˜å…ˆåŒ¹é…é•¿çš„å…³é”®è¯ï¼Œé¿å…è¯¯åŒ¹é…ï¼‰
        all_keywords = []
        for qtype, keywords in self.keywords_map.items():
            for keyword in keywords:
                all_keywords.append((len(keyword), keyword.lower(), qtype))
        
        # æŒ‰é•¿åº¦é™åºæ’åˆ—ï¼Œè¿™æ ·æ›´é•¿çš„å…³é”®è¯ä¼šä¼˜å…ˆåŒ¹é…
        all_keywords.sort(reverse=True)
        
        for length, keyword, qtype in all_keywords:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…ï¼šå…³é”®è¯å‰åéƒ½æ˜¯ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·
            if re.search(r'(^|\s|[ï¼Œã€‚ï¼ï¼Ÿ])' + re.escape(keyword) + r'($|\s|[ï¼Œã€‚ï¼ï¼Ÿ])', question_lower):
                return qtype
            # å¦‚æœä¸Šé¢çš„ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…³é”®è¯åŒ¹é…ï¼ˆä½†åªå¯¹äºè¾ƒé•¿çš„å…³é”®è¯ï¼‰
            if len(keyword) >= 3 and keyword in question_lower:
                return qtype
        
        # é»˜è®¤åˆ†ç±»
        return "general_inquiry"

class ContextBuilder:
    """ä¸Šä¸‹æ–‡æ„å»ºå™¨"""
    
    def __init__(self, settings: Settings):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        self.settings = settings
    
    async def build(
        self,
        question: str,
        user_id: str,
        question_type: str,
        extra_context: Optional[Dict[str, Any]] = None
    ) -> ContextData:
        """
        æ„å»ºé—®é¢˜å¤„ç†çš„ä¸Šä¸‹æ–‡
        
        å‚æ•°ï¼š
        - question: é—®é¢˜æ–‡æœ¬
        - user_id: ç”¨æˆ·ID
        - question_type: é—®é¢˜ç±»å‹
        - extra_context: é¢å¤–ä¸Šä¸‹æ–‡
        
        è¿”å›ï¼š
        - context: ContextDataå¯¹è±¡
        """
        context = ContextData(
            user_profile={
                "user_id": user_id,
                "question_type": question_type,
            },
            extra=extra_context or {}
        )
        
        # ä»extra_contextä¸­æå–éƒ¨é—¨å’Œè§’è‰²ä¿¡æ¯
        if extra_context:
            context.department = extra_context.get("department")
            context.role = extra_context.get("role")
            context.permissions = extra_context.get("permissions", [])
        
        logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ - ç”¨æˆ·: {user_id}, ç±»å‹: {question_type}")
        
        return context

class QAProcessor:
    """QAå¤„ç†å™¨ - åè°ƒå„ä¸ªæœåŠ¡"""
    
    def __init__(self, settings: Settings, redis):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.settings = settings
        self.redis = redis
    
    async def process(
        self,
        question_id: str,
        question: str,
        question_type: str,
        context: ContextData,
        user_id: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†é—®é¢˜çš„ä¸»æµç¨‹
        
        1. æ£€æŸ¥ç¼“å­˜
        2. è°ƒç”¨RAGæ£€ç´¢çŸ¥è¯†åº“
        3. è°ƒç”¨Agentè·å–å®æ—¶æ•°æ®
        4. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        5. è¿”å›ç»“æœ
        """
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cached_result = self._check_cache(question)
        if cached_result:
            logger.info(f"[{question_id}] å‘½ä¸­ç¼“å­˜")
            return cached_result
        
        # 2. RAGæ£€ç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰
        rag_results = await self._call_rag(question, question_type)
        
        # 3. Agentè°ƒç”¨ï¼ˆæ¨¡æ‹Ÿï¼‰
        agent_results = await self._call_agent(question, question_type, context)
        
        # 4. ç»„è£…ç­”æ¡ˆ
        answer = await self._generate_answer(
            question=question,
            rag_results=rag_results,
            agent_results=agent_results,
            context=context
        )
        
        result = {
            "answer": answer,
            "sources": rag_results.get("sources", []) + agent_results.get("sources", []),
            "confidence": max(
                rag_results.get("confidence", 0),
                agent_results.get("confidence", 0)
            )
        }
        
        # 5. ç¼“å­˜ç»“æœ
        self._cache_result(question, result)
        
        return result
    
    def _check_cache(self, question: str) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥ç¼“å­˜"""
        try:
            cache_key = f"qa_cache:{hash(question) % 10000}"
            cached = self.redis.get(cache_key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        return None
    
    def _cache_result(self, question: str, result: Dict[str, Any]):
        """ç¼“å­˜ç»“æœ"""
        try:
            cache_key = f"qa_cache:{hash(question) % 10000}"
            self.redis.setex(
                cache_key,
                86400,  # 24å°æ—¶
                json.dumps(result, ensure_ascii=False)
            )
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    async def _call_rag(self, question: str, question_type: str) -> Dict[str, Any]:
        """è°ƒç”¨ RAG æœåŠ¡æ£€ç´¢çŸ¥è¯†åº“"""
        logger.info(f"ğŸ“š è°ƒç”¨ RAG æœåŠ¡æŸ¥è¯¢: {question}")
        
        try:
            async with aiohttp.ClientSession() as session:
                # æ ¹æ®é—®é¢˜ç±»å‹ç¡®å®šæœç´¢åˆ†ç±»
                category_map = {
                    "sales_inquiry": "sales",
                    "hr_inquiry": "hr",
                    "technical_inquiry": "technical",
                    "financial_inquiry": "finance",
                    "customer_inquiry": "case_study"
                }
                category = category_map.get(question_type)
                
                search_payload = {
                    "query": question,
                    "top_k": 3,
                    "category": category,
                    "threshold": 0.5
                }
                
                async with session.post(
                    "http://rag_service:8000/api/rag/search",
                    json=search_payload,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as resp:
                    if resp.status != 200:
                        logger.warning(f"âš ï¸ RAG æœåŠ¡è¿”å›é”™è¯¯: {resp.status}")
                        return {
                            "sources": [],
                            "content": "",
                            "confidence": 0.0,
                            "retrieval_status": "failed"
                        }
                    
                    data = await resp.json()
                    documents = data.get("documents", [])
                    
                    if not documents:
                        logger.info(f"âŒ çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                        return {
                            "sources": [],
                            "content": "",
                            "confidence": 0.0,
                            "retrieval_status": "no_results",
                            "search_hint": "å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–æŸ¥çœ‹FAQéƒ¨åˆ†"
                        }
                    
                    # æå–æ–‡æ¡£å†…å®¹
                    contents = [doc.get("content", "") for doc in documents if isinstance(doc, dict)]
                    sources = [doc.get("source", "") for doc in documents if isinstance(doc, dict)]
                    
                    combined_content = "\n".join(contents[:2])  # æœ€å¤šå–2ä¸ªæ–‡æ¡£
                    
                    logger.info(f"âœ… çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    return {
                        "sources": sources,
                        "content": combined_content,
                        "confidence": 0.85,
                        "retrieval_status": "success",
                        "documents_count": len(documents)
                    }
        
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ RAG æœåŠ¡è¶…æ—¶")
            return {
                "sources": [],
                "content": "",
                "confidence": 0.0,
                "retrieval_status": "timeout"
            }
        except Exception as e:
            logger.error(f"ğŸ”´ è°ƒç”¨ RAG æœåŠ¡å‡ºé”™: {str(e)}")
            return {
                "sources": [],
                "content": "",
                "confidence": 0.0,
                "retrieval_status": "error",
                "error_message": str(e)
            }
    
    async def _call_agent(self, question: str, question_type: str, context: ContextData) -> Dict[str, Any]:
        """è°ƒç”¨AgentæœåŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        logger.info(f"è°ƒç”¨AgentæœåŠ¡æ‰§è¡Œ: {question}")
        
        # æ¨¡æ‹ŸAgentè°ƒç”¨ä¼ä¸šç³»ç»Ÿ
        if "sales" in question_type:
            return {
                "sources": ["erp_system"],
                "content": "Q1é”€å”®æ•°æ®: 5000ä¸‡å…ƒï¼ŒåŒæ¯”å¢é•¿15%",
                "confidence": 0.95
            }
        elif "hr" in question_type:
            return {
                "sources": ["hr_system"],
                "content": "å½“å‰å‘˜å·¥æ€»æ•°: 500äººï¼Œæœ¬æœˆå…¥èŒ: 10äºº",
                "confidence": 0.90
            }
        else:
            return {
                "sources": [],
                "content": "",
                "confidence": 0.0
            }
    
    async def _generate_answer(
        self,
        question: str,
        rag_results: Dict[str, Any],
        agent_results: Dict[str, Any],
        context: ContextData
    ) -> str:
        """ç”Ÿæˆç­”æ¡ˆ - è°ƒç”¨çœŸå®LLM"""
        logger.info("è°ƒç”¨çœŸå®LLMç”Ÿæˆç­”æ¡ˆ...")
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
        has_knowledge = bool(rag_results.get("content"))
        has_agent_data = bool(agent_results.get("content"))
        
        # ç»„è£…ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        sources = []
        
        if agent_results.get("content"):
            context_parts.append(f"ä¼ä¸šæ•°æ®: {agent_results['content']}")
            sources.extend(agent_results.get("sources", []))
        
        if rag_results.get("content"):
            context_parts.append(f"çŸ¥è¯†åº“å†…å®¹: {rag_results['content']}")
            sources.extend(rag_results.get("sources", []))
        
        # æ ‡è®°æ£€ç´¢è¿‡ç¨‹
        retrieval_status = rag_results.get("retrieval_status", "unknown")
        
        # æ„é€ è°ƒç”¨ LLM çš„ prompt
        if has_knowledge or has_agent_data:
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¼ä¸šAIåŠ©æ‰‹ã€‚
è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

{chr(10).join(context_parts)}

æ•°æ®æ¥æº: {', '.join(sources)}

è¯·æä¾›æ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚"""
        else:
            # å³ä½¿æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†åº“ä¿¡æ¯ï¼Œä¹Ÿè®© LLM å°è¯•å›ç­”
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¼ä¸šAIåŠ©æ‰‹ã€‚
ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œä½†çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚
è¯·æ ¹æ®ä½ çš„çŸ¥è¯†åŸºç¡€æä¾›ä¸€ä¸ªæœ‰å¸®åŠ©çš„ç­”æ¡ˆã€‚
å¦‚æœéœ€è¦ï¼Œå¯ä»¥å»ºè®®ç”¨æˆ·è”ç³»ç›¸å…³éƒ¨é—¨ä»¥è·å¾—æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚"""

        user_prompt = question
        
        try:
            # è°ƒç”¨çœŸå® LLM API
            answer = await self._call_real_llm(system_prompt, user_prompt)
            logger.info(f"âœ… LLM ç”Ÿæˆç­”æ¡ˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)}")
            
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ç»“æœï¼Œæ·»åŠ è¯´æ˜
            if not has_knowledge and not has_agent_data:
                answer = f"ğŸ“ åŸºäºé€šç”¨çŸ¥è¯†åº“çš„å›ç­”ï¼ˆçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼‰ï¼š\n\n{answer}"
            
            return answer
        except Exception as e:
            logger.error(f"ğŸ”´ LLM è°ƒç”¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡é€‰ç­”æ¡ˆ")
            # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›å¸¦æç¤ºçš„ç®€å•ç­”æ¡ˆ
            if has_knowledge or has_agent_data:
                simple_answer = "æ ¹æ®æˆ‘ä»¬æŒæ¡çš„ä¿¡æ¯ï¼š\n"
                if agent_results.get("content"):
                    simple_answer += f"\nä¼ä¸šæ•°æ®åé¦ˆ: {agent_results['content']}"
                if rag_results.get("content"):
                    simple_answer += f"\nçŸ¥è¯†åº“ä¿¡æ¯: {rag_results['content']}"
                simple_answer += f"\n\nğŸ“Š æ•°æ®æ¥æº: {', '.join(sources)}"
                return simple_answer
            else:
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ç”¨å…¶ä»–å…³é”®è¯æé—®ï¼Œæˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
    
    async def _call_real_llm(self, system_prompt: str, user_prompt: str) -> str:
        """
        è°ƒç”¨çœŸå® LLM æœåŠ¡
        
        ä» LLM Service è¯»å–å½“å‰é…ç½®ï¼ˆprovider å’Œ API Keyï¼‰
        æ”¯æŒ OpenAI å’Œ ChatAnywhere
        """
        try:
            # 1. åˆ›å»ºæ–°çš„ session ç”¨äº LLM è°ƒç”¨
            async with aiohttp.ClientSession() as session:
                # 1a. ä» LLM Service è·å–å½“å‰é…ç½®
                async with session.get(
                    "http://llm_service:8000/api/llm/config",
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"è·å– LLM é…ç½®å¤±è´¥ ({resp.status}): {error_text}")
                    
                    config = await resp.json()
                    provider = config.get("provider", "openai")
                    model = config.get("model", "gpt-3.5-turbo")
                    status = config.get("status", "")
                    
                    if status == "not_configured":
                        raise Exception(f"LLM æä¾›å•† {provider} æœªé…ç½® API Key")
                    
                    logger.info(f"ğŸ¤– ä½¿ç”¨å·²å¯ç”¨çš„ LLM: {provider.upper()}, æ¨¡å‹: {model}")
                
                # 2. è°ƒç”¨ LLM æœåŠ¡çš„ chat æ¥å£ï¼ˆLLM Service ä¼šæ ¹æ®é…ç½®ä½¿ç”¨æ­£ç¡®çš„æä¾›å•†ï¼‰
                payload = {
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "model": model,
                    "temperature": 0.7,
                    "max_tokens": 2048
                }
                
                async with session.post(
                    "http://llm_service:8000/api/llm/chat",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"LLM æœåŠ¡è¿”å›é”™è¯¯ ({resp.status}): {error_text}")
                    
                    result = await resp.json()
                    answer = result.get("content") or ""
                    
                    if not answer:
                        raise Exception("LLM è¿”å›ç©ºçš„ç­”æ¡ˆ")
                    
                    tokens = result.get("tokens_used", 0)
                    logger.info(f"âœ… {provider.upper()} è¿”å›ç­”æ¡ˆï¼Œæ¶ˆè€— tokens: {tokens}")
                    return answer
        
        except asyncio.TimeoutError:
            logger.error("â±ï¸ LLM æœåŠ¡è¯·æ±‚è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
            raise Exception("LLM æœåŠ¡è¯·æ±‚è¶…æ—¶")
        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨ LLM å‡ºé”™: {str(e)}")
            raise
    
    async def _call_openai_llm(self, system_prompt: str, user_prompt: str, model_info: Dict[str, Any]) -> str:
        """
        è°ƒç”¨ OpenAI API
        
        æ”¯æŒ GPT-4, GPT-3.5-turbo ç­‰æ¨¡å‹
        """
        try:
            import openai
            
            api_key = model_info.get("api_key")
            if not api_key or api_key == "sk-":
                raise Exception("OpenAI API Key æœªé…ç½®")
            
            # è®¾ç½® API key
            openai.api_key = api_key
            
            # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹åç§°
            model_name = model_info.get("name", "gpt-3.5-turbo").lower()
            if "gpt-4" in model_name:
                model = "gpt-4"
            else:
                model = "gpt-3.5-turbo"
            
            logger.info(f"ğŸ“¤ è°ƒç”¨ OpenAI APIï¼Œæ¨¡å‹: {model}")
            
            response = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=model_info.get("temperature", 0.7),
                max_tokens=min(model_info.get("max_tokens", 2048), 4096),
                timeout=30
            )
            
            answer = response['choices'][0]['message']['content']
            tokens = response['usage']['total_tokens']
            logger.info(f"ğŸ“¥ OpenAI è¿”å›: tokens={tokens}")
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def _call_chatanywhere_llm(self, system_prompt: str, user_prompt: str, model_info: Dict[str, Any]) -> str:
        """
        è°ƒç”¨ ChatAnywhere API
        
        ChatAnywhere æä¾›å…è´¹çš„ ChatGPT APIï¼Œå…¼å®¹ OpenAI æ¥å£
        GitHub: https://github.com/chatanywhere/ChatGPT_API_free
        
        API ç«¯ç‚¹: https://api.chatanywhere.com.cn/v1/chat/completions
        è®¤è¯æ–¹å¼: Bearer Token (API Key)
        
        é…ç½®ç¤ºä¾‹:
        - api_key: your-chatanywhere-api-key (ä» https://chatanywhere.com.cn/ è·å–)
        - name: ChatGPT / GPT-4 / Claude ç­‰
        - temperature: 0.7
        - max_tokens: 2048
        """
        try:
            import openai
            
            api_key = model_info.get("api_key")
            if not api_key:
                raise Exception("ChatAnywhere API Key æœªé…ç½®ï¼Œè¯·è®¿é—® https://chatanywhere.com.cn/ è·å–")
            
            # ChatAnywhere å…¼å®¹ OpenAI æ¥å£ï¼Œéœ€è¦é…ç½®è‡ªå®šä¹‰ç«¯ç‚¹
            openai.api_key = api_key
            openai.api_base = "https://api.chatanywhere.com.cn/v1"
            
            # è·å–æ¨¡å‹åç§°ï¼Œæ”¯æŒ gpt-3.5-turbo, gpt-4, claude ç­‰
            model_name = model_info.get("name", "gpt-3.5-turbo").lower()
            
            # ChatAnywhere æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
            supported_models = {
                "gpt-4": "gpt-4",
                "gpt-3.5-turbo": "gpt-3.5-turbo",
                "gpt-3.5": "gpt-3.5-turbo",
                "chatgpt": "gpt-3.5-turbo",
                "claude": "claude-3-opus",  # å¦‚æœæ”¯æŒ Claude
            }
            
            # åŒ¹é…æ¨¡å‹åç§°
            model = "gpt-3.5-turbo"  # é»˜è®¤æ¨¡å‹
            for key, value in supported_models.items():
                if key in model_name:
                    model = value
                    break
            
            logger.info(f"ğŸ“¤ è°ƒç”¨ ChatAnywhere APIï¼Œæ¨¡å‹: {model}")
            
            response = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=model_info.get("temperature", 0.7),
                max_tokens=min(model_info.get("max_tokens", 2048), 4096),
                timeout=30
            )
            
            answer = response['choices'][0]['message']['content']
            tokens = response.get('usage', {}).get('total_tokens', 0)
            logger.info(f"ğŸ“¥ ChatAnywhere è¿”å›: tokens={tokens}")
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ ChatAnywhere API è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
